{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d08eb72",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/vm/sl1j2v913dd5mfx0sqw5pmch0000gn/T/ipykernel_28703/620410904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN%3Aen'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msleep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mscrolldown\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhtml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'a.WwrzSb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marticle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests_html.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, retries, script, wait, scrolldown, sleep, reload, timeout, keep_page)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \"\"\"\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m  \u001b[0;31m# Automatically create a event loop and browser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m         \u001b[0mcontent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/requests_html.py\u001b[0m in \u001b[0;36mbrowser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masyncio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_event_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_browser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_until_complete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_browser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Cannot use HTMLSession within an existing event loop. Use AsyncHTMLSession instead."
     ]
    }
   ],
   "source": [
    "from requests_html import HTMLSession\n",
    "session = HTMLSession()\n",
    "url = 'https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "r = session.get(url)\n",
    "r.html.render(sleep = 1 , scrolldown = 1)\n",
    "articles = r.html.find('a.WwrzSb')\n",
    "for article in articles:\n",
    "    print(article.text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44792e9d-2195-41db-8ee0-707763578e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://news.google.com./articles/CBMiiwFodHRwczovL3RpbWVzb2ZpbmRpYS5pbmRpYXRpbWVzLmNvbS9pbmRpYS9hYXAtY29uZ3Jlc3MtYW5ub3VuY2UtZGVsaGktYWxsaWFuY2UtZm9yLWxzLXBvbGxzLXNob3VsZC1ianAtYmUtd29ycmllZC9hcnRpY2xlc2hvdy8xMDc5NzI3ODAuY21z0gEA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Heading of the first article not found.\n"
     ]
    }
   ],
   "source": [
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "async def main():\n",
    "    session = AsyncHTMLSession()\n",
    "    url = 'https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "    r = await session.get(url)\n",
    "    await r.html.arender(sleep=1, scrolldown=1)\n",
    "    \n",
    "    articles = r.html.find('a.WwrzSb')\n",
    "    article_urls = []  # List to store full URLs of articles\n",
    "    for article in articles:\n",
    "        href = article.attrs['href']  # Extract the href attribute\n",
    "        full_url = 'https://news.google.com' + href  # Construct the full URL\n",
    "        article_urls.append(full_url)  # Append full URL to the list\n",
    "    # Now scrape articles from the stored URLs\n",
    "\n",
    "# Run the async function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "737f20ed-3995-4045-ba9f-7e48a12c2516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/opt/anaconda3/lib/python3.9/site-packages/pyppeteer/util.py:29: RuntimeWarning: coroutine 'scrape_article' was never awaited\n",
      "  gc.collect()\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Article content not found for: https://news.google.com./articles/CBMijAFodHRwczovL3d3dy5oaW5kdXN0YW50aW1lcy5jb20vaW5kaWEtbmV3cy9yZXZlbmdlLW9mLXRoZS1wcmluY2UtYmpwLW9uLWNvbmdyZXNzLWdpdmluZy1hd2F5LWFobWVkLXBhdGVscy1iaGFydWNoLXRvLWFhcC0xMDE3MDg3NzQ2MzQyMTAuaHRtbNIBAA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Error scraping article from: https://news.google.com./articles/CBMijQFodHRwczovL3d3dy5uZHR2LmNvbS9pbmRpYS1uZXdzL2FhcC1jb25ncmVzcy1ianAtZGVsaGktbWFub2otdGl3YXJpLWNob3ItY2hvci1tYXVzZXJlLWJoYWktYmpwLWxlYWRlci1hcy1hYXAtY29uZ3Jlc3Mtc2VhbC1kZWxoaS1kZWFsLTUxMTgzNTLSAZMBaHR0cHM6Ly93d3cubmR0di5jb20vaW5kaWEtbmV3cy9hYXAtY29uZ3Jlc3MtYmpwLWRlbGhpLW1hbm9qLXRpd2FyaS1jaG9yLWNob3ItbWF1c2VyZS1iaGFpLWJqcC1sZWFkZXItYXMtYWFwLWNvbmdyZXNzLXNlYWwtZGVsaGktZGVhbC01MTE4MzUyL2FtcC8x?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Exception: Navigation Timeout Exceeded: 15000 ms exceeded.\n",
      "Error scraping article from: https://news.google.com./articles/CBMiqQFodHRwczovL3d3dy5saXZlbWludC5jb20vcG9saXRpY3MvbG9rLXNhYmhhLWVsZWN0aW9ucy0yMDI0LWZ1bGwtbGlzdC1vZi1zZWF0cy10by1iZS1jb250ZXN0ZWQtYnktYWFwLWNvbmdyZXNzLWFsbGlhbmNlLXdoaWNoLXNlYXQtZ29lcy10by13aGljaC1wYXJ0eS0xMTcwODc2NTc0OTUxMi5odG1s0gEA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Exception: Navigation Timeout Exceeded: 15000 ms exceeded.\n",
      "Article content not found for: https://news.google.com./articles/CBMiowFodHRwczovL3d3dy5saXZlbWludC5jb20vbmV3cy93b3JsZC92bGFkaW1pci1wdXRpbnMtY3JpdGljLWFsZXhlaS1uYXZhbG55cy1ib2R5LWhhbmRlZC10by1oaXMtbW90aGVyLWRheXMtYWZ0ZXItaGUtZGllZC1pbi1hcmN0aWMtcHJpc29uLXJ1c3NpYS0xMTcwODc5Mjk0NTM1Ny5odG1s0gGnAWh0dHBzOi8vd3d3LmxpdmVtaW50LmNvbS9uZXdzL3dvcmxkL3ZsYWRpbWlyLXB1dGlucy1jcml0aWMtYWxleGVpLW5hdmFsbnlzLWJvZHktaGFuZGVkLXRvLWhpcy1tb3RoZXItZGF5cy1hZnRlci1oZS1kaWVkLWluLWFyY3RpYy1wcmlzb24tcnVzc2lhL2FtcC0xMTcwODc5Mjk0NTM1Ny5odG1s?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Error scraping article from: https://news.google.com./articles/CBMid2h0dHBzOi8vd3d3LmhpbmR1c3RhbnRpbWVzLmNvbS93b3JsZC1uZXdzL2FsZXhlaS1uYXZhbG55cy1ib2R5LWhhbmRlZC1vdmVyLXRvLWhpcy1tb3RoZXItYWlkZS1zYXlzLTEwMTcwODc5MTg1NjkzOC5odG1s0gEA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Exception: Navigation Timeout Exceeded: 15000 ms exceeded.\n",
      "Error scraping article from: https://news.google.com./articles/CBMilQFodHRwczovL3d3dy5uZHR2LmNvbS93b3JsZC1uZXdzL3B1dGluLWNyaXRpYy1hbGV4ZWktbmF2YWxueS1tYXktaGF2ZS1iZWVuLWtpbGxlZC13aXRoLXNpbmdsZS1wdW5jaC10by1oZWFydC1jbGFpbXMtYWN0aXZpc3QtdmxhZGltaXItb3NlY2hraW4tNTExOTE5OdIBAA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Exception: Navigation Timeout Exceeded: 15000 ms exceeded.\n",
      "Error scraping article from: https://news.google.com./articles/CBMiiwFodHRwczovL3RpbWVzb2ZpbmRpYS5pbmRpYXRpbWVzLmNvbS9pbmRpYS9hYXAtY29uZ3Jlc3MtYW5ub3VuY2UtZGVsaGktYWxsaWFuY2UtZm9yLWxzLXBvbGxzLXNob3VsZC1ianAtYmUtd29ycmllZC9hcnRpY2xlc2hvdy8xMDc5NzI3ODAuY21z0gEA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Exception: Navigation Timeout Exceeded: 15000 ms exceeded.\n",
      "Error scraping article from: https://news.google.com./articles/CBMilQFodHRwczovL3RpbWVzb2ZpbmRpYS5pbmRpYXRpbWVzLmNvbS93b3JsZC9ldXJvcGUvcHV0aW5zLWNyaXRpYy1uYXZhbG55cy1ib2R5LWhhbmRlZC1vdmVyLXRvLWhpcy1tb3RoZXItd2Vlay1hZnRlci1oaXMtZGVhdGgvYXJ0aWNsZXNob3cvMTA3OTcyNDU1LmNtc9IBAA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Exception: Navigation Timeout Exceeded: 15000 ms exceeded.\n",
      "Error scraping article from: https://news.google.com./articles/CBMifmh0dHBzOi8vdGhld2lyZS5pbi9nb3Zlcm5tZW50L3VwLXBvbGljZS1yZWNydWl0bWVudC1leGFtLWNhbmNlbGxlZC1ianAteWllbGRzLXRvLXByZXNzdXJlLWFtaWQtcGFwZXItbGVhay1hbGxlZ2F0aW9ucy1wcm90ZXN0c9IBjAFodHRwczovL20udGhld2lyZS5pbi9hcnRpY2xlL2dvdmVybm1lbnQvdXAtcG9saWNlLXJlY3J1aXRtZW50LWV4YW0tY2FuY2VsbGVkLWJqcC15aWVsZHMtdG8tcHJlc3N1cmUtYW1pZC1wYXBlci1sZWFrLWFsbGVnYXRpb25zLXByb3Rlc3RzL2FtcA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Exception: Navigation Timeout Exceeded: 15000 ms exceeded.\n",
      "Error scraping article from: https://news.google.com./articles/CBMilQFodHRwczovL3d3dy5oaW5kdXN0YW50aW1lcy5jb20vY2l0aWVzL2RlaHJhZHVuLW5ld3MvaGFsZHdhbmktdmlvbGVuY2UtYWNjdXNlZC1hYmR1bC1tYWlsay1hcnJlc3RlZC1mcm9tLWRlbGhpLXV0dGFyYWtoYW5kLXBvbGljZS0xMDE3MDg3ODQyMDgxMTcuaHRtbNIBAA?hl=en-IN&gl=IN&ceid=IN%3Aen\n",
      "Exception: Navigation Timeout Exceeded: 15000 ms exceeded.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "async def scrape_article(session, article_url):\n",
    "    try:\n",
    "        r = await session.get(article_url)\n",
    "        await r.html.arender(sleep=1, scrolldown=1, timeout=15)\n",
    "\n",
    "        # Extract and print the content of the article\n",
    "        article_content = r.html.find('div.article-content', first=True)\n",
    "        if article_content:\n",
    "            print(\"Article content from:\", article_url)\n",
    "            print(article_content.text)\n",
    "        else:\n",
    "            print(\"Article content not found for:\", article_url)\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping article from:\", article_url)\n",
    "        print(\"Exception:\", str(e))\n",
    "\n",
    "async def main():\n",
    "    session = AsyncHTMLSession()\n",
    "    url = 'https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "    try_count = 0\n",
    "    max_tries = 3\n",
    "    while try_count < max_tries:\n",
    "        try:\n",
    "            r = await session.get(url)\n",
    "            await r.html.arender(sleep=1, scrolldown=1, timeout=15)\n",
    "            break  # Break out of the loop if successful\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {try_count + 1} failed:\", str(e))\n",
    "            try_count += 1\n",
    "    else:\n",
    "        print(\"Max retries exceeded. Unable to load the page.\")\n",
    "        return  # Exit the function if max retries are exceeded\n",
    "\n",
    "    articles = r.html.find('a.WwrzSb')\n",
    "    article_urls = []  # List to store full URLs of articles\n",
    "    for article in articles:\n",
    "        href = article.attrs['href']  # Extract the href attribute\n",
    "        full_url = 'https://news.google.com' + href  # Construct the full URL\n",
    "        article_urls.append(full_url)  # Append full URL to the list\n",
    "\n",
    "    # Now scrape articles from the stored URLs\n",
    "    tasks = []\n",
    "    for article_url in article_urls:\n",
    "        task = scrape_article(session, article_url)\n",
    "        tasks.append(task)\n",
    "\n",
    "    # Wait for all tasks to complete\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# Run the async function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b68a4e1-5a62-45a8-ad13-eef17f34b663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading main page: Navigation Timeout Exceeded: 15000 ms exceeded.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from requests_html import AsyncHTMLSession\n",
    "\n",
    "async def scrape_article(session, article_url, max_retries=3):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            r = await session.get(article_url)\n",
    "            await r.html.arender(sleep=1, scrolldown=1, timeout=30)  # Increased timeout\n",
    "            article_content = r.html.find('div.article-content', first=True)\n",
    "            if article_content:\n",
    "                print(\"Article content from:\", article_url)\n",
    "                print(article_content.text)\n",
    "            else:\n",
    "                print(\"Article content not found for:\", article_url)\n",
    "            return  # Exit function if successful\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping article ({attempt + 1}/{max_retries}): {str(e)}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"Retrying...\")\n",
    "            else:\n",
    "                print(\"Max retries exceeded. Unable to scrape article.\")\n",
    "\n",
    "async def main():\n",
    "    session = AsyncHTMLSession()\n",
    "    url = 'https://news.google.com/home?hl=en-IN&gl=IN&ceid=IN%3Aen'\n",
    "    try:\n",
    "        r = await session.get(url)\n",
    "        await r.html.arender(sleep=1, scrolldown=1, timeout=30)\n",
    "    except Exception as e:\n",
    "        print(\"Error loading main page:\", str(e))\n",
    "        return  # Exit function if unable to load main page\n",
    "\n",
    "    articles = r.html.find('a.WwrzSb')\n",
    "    article_urls = []\n",
    "    for article in articles:\n",
    "        href = article.attrs['href']\n",
    "        full_url = 'https://news.google.com' + href\n",
    "        article_urls.append(full_url)\n",
    "\n",
    "    tasks = []\n",
    "    for article_url in article_urls:\n",
    "        task = scrape_article(session, article_url)\n",
    "        tasks.append(task)\n",
    "\n",
    "    await asyncio.gather(*tasks)\n",
    "\n",
    "# Run the async function\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23da7a9d-7d21-467b-9747-5162c417951b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
